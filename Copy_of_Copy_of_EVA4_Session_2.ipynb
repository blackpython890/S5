{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of EVA4 - Session 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagatabhay/S5/blob/master/Copy_of_Copy_of_EVA4_Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqFocHtyYrHv",
        "colab_type": "text"
      },
      "source": [
        "**Target**: create an architecture with less than 10k parameters so accordingly chooses the imput and output channels.Here I have choosen the channels between 10 and 20.\n",
        "\n",
        "**Analysis**: Adding dropout after max pooling has yielded better results in terms\n",
        "of Accuracy rather than after each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input -? OUtput? RF\n",
        "        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        #self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        #self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        #self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        #self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        #self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "        #self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "        #self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=1,out_channels=10,kernel_size=(3,3),padding=0),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.BatchNorm2d(num_features=10),\n",
        "                                   #nn.Dropout(0.1)\n",
        "                                   )\n",
        "\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels=10,out_channels=16,kernel_size=(3,3),padding=0),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.BatchNorm2d(num_features=16),\n",
        "                                   #nn.Dropout(0.1)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_channels=16,out_channels=20,kernel_size=(3,3),padding=0),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.BatchNorm2d(num_features=20),\n",
        "                                   #nn.Dropout(0.1)\n",
        "                                   )                          \n",
        "        \n",
        "        self.pool1 =  nn.MaxPool2d(2,2)\n",
        "         \n",
        "        \n",
        "        \n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_channels=20,out_channels=10,kernel_size=(1,1),padding=0),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.BatchNorm2d(num_features=10),\n",
        "                                   nn.Dropout(0.1)\n",
        "                                   ) \n",
        "        \n",
        "        #self.pool2 =  nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.conv5 = nn.Sequential(nn.Conv2d(in_channels=10,out_channels=20,kernel_size=(3,3),padding=0),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.BatchNorm2d(num_features=20),\n",
        "                                   #nn.Dropout(0.1)\n",
        "                                   ) \n",
        "        \n",
        "        self.conv6 = nn.Sequential(nn.Conv2d(in_channels=20,out_channels=16,kernel_size=(3,3),padding=0),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.BatchNorm2d(num_features=16),\n",
        "                                   #nn.Dropout(0.1)\n",
        "                                   ) \n",
        "        self.conv7 = nn.Sequential(nn.Conv2d(in_channels=16,out_channels=10,kernel_size=(1,1),padding=0)\n",
        "                                   #nn.ReLU()\n",
        "                                   #nn.BatchNorm2d(num_features=10)\n",
        "                                   #nn.DroupOut2d()\n",
        "                                   )\n",
        "        self.gap  =  nn.Sequential(nn.AvgPool2d(kernel_size=7)) \n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "              \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "        #x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        #x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        #x = F.relu(self.conv7(x))\n",
        "        #x = x.view(-1, 10)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv4(x)\n",
        "        #x = self.pool2(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        \n",
        "        x = self.conv7(x)\n",
        "        x = self.gap(x)\n",
        "        \n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb4U7hLWZAXp",
        "colab_type": "text"
      },
      "source": [
        "Summary of our model architecture.\n",
        "\n",
        "1. Used Batch Normalization at every layer and dropout after max pooling layer\n",
        "\n",
        "2. Used GAP of 7*7 at the end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xdydjYTZFyi3",
        "outputId": "3d1a82a2-be17-4f45-e660-bc27b3b713d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 26, 26]             100\n",
            "              ReLU-2           [-1, 10, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 10, 26, 26]              20\n",
            "            Conv2d-4           [-1, 16, 24, 24]           1,456\n",
            "              ReLU-5           [-1, 16, 24, 24]               0\n",
            "       BatchNorm2d-6           [-1, 16, 24, 24]              32\n",
            "            Conv2d-7           [-1, 20, 22, 22]           2,900\n",
            "              ReLU-8           [-1, 20, 22, 22]               0\n",
            "       BatchNorm2d-9           [-1, 20, 22, 22]              40\n",
            "        MaxPool2d-10           [-1, 20, 11, 11]               0\n",
            "          Dropout-11           [-1, 20, 11, 11]               0\n",
            "           Conv2d-12           [-1, 10, 11, 11]             210\n",
            "             ReLU-13           [-1, 10, 11, 11]               0\n",
            "      BatchNorm2d-14           [-1, 10, 11, 11]              20\n",
            "          Dropout-15           [-1, 10, 11, 11]               0\n",
            "           Conv2d-16             [-1, 20, 9, 9]           1,820\n",
            "             ReLU-17             [-1, 20, 9, 9]               0\n",
            "      BatchNorm2d-18             [-1, 20, 9, 9]              40\n",
            "           Conv2d-19             [-1, 16, 7, 7]           2,896\n",
            "             ReLU-20             [-1, 16, 7, 7]               0\n",
            "      BatchNorm2d-21             [-1, 16, 7, 7]              32\n",
            "           Conv2d-22             [-1, 10, 7, 7]             170\n",
            "        AvgPool2d-23             [-1, 10, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 9,736\n",
            "Trainable params: 9,736\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.72\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.76\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:96: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI0pU9G0ZQm5",
        "colab_type": "text"
      },
      "source": [
        "Apply RamdomRotation to image by 7 degree for getting better accuracy tried \n",
        "\n",
        "with other degrees like 5,6 and 10 this was best . \n",
        "\n",
        " converting the image to tensor and normalizing the pixel of image.Used Random Affine image augmentation techniques.\n",
        "\n",
        "We are doing image Augmentation on the train dataset not on testing dataset.\n",
        "Used ColorJitter with different combination of parameters as well as resize function of transform but didnot gave better results in terms of accuracy so commented it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "batch_size = 30\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        #transforms.Resize((30,30)),                          \n",
        "                        transforms.RandomRotation((-7.0,7.0),fill=(1,)),\n",
        "                        transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9, 1.1)),\n",
        "                        #transforms.ColorJitter(brightness=0.1, contrast=0.1,saturation=0.1,hue=0.1),                          \n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "train_losses=[]\n",
        "#test_losses=[]\n",
        "train_acc=[]\n",
        "test_acc=[]\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(data)\n",
        "        loss = F.nll_loss(y_pred, target)\n",
        "        train_losses.append(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred=y_pred.argmax(dim=1,keepdim=True)\n",
        "        correct+=pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed+=len(data)\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "        train_acc.append(100*correct/processed)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlqX1HW0bIlX",
        "colab_type": "text"
      },
      "source": [
        "Using epoch of 15.Using StepLR function from lr schedular module using step_size of 5 is giving better accuracy compared to 4 and 6.This is basically done by Hit and Trial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab_type": "code",
        "outputId": "e806e912-3e37-4406-8e28-7334b50aa102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler= StepLR(optimizer,step_size=5,gamma=0.1)\n",
        "\n",
        "for epoch in range(1, 15):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:96: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=0.1441677063703537 batch_id=1999 Accuracy=88.55: 100%|██████████| 2000/2000 [00:47<00:00, 41.94it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0658, Accuracy: 9798/10000 (97.98%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.04076778143644333 batch_id=1999 Accuracy=95.92: 100%|██████████| 2000/2000 [00:47<00:00, 42.00it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0417, Accuracy: 9865/10000 (98.65%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.14718104898929596 batch_id=1999 Accuracy=96.73: 100%|██████████| 2000/2000 [00:47<00:00, 41.74it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0326, Accuracy: 9903/10000 (99.03%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.060300540179014206 batch_id=1999 Accuracy=97.02: 100%|██████████| 2000/2000 [00:48<00:00, 41.44it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0320, Accuracy: 9891/10000 (98.91%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.05168073996901512 batch_id=1999 Accuracy=97.32: 100%|██████████| 2000/2000 [00:48<00:00, 41.19it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0308, Accuracy: 9892/10000 (98.92%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.05107460170984268 batch_id=1999 Accuracy=97.54: 100%|██████████| 2000/2000 [00:48<00:00, 41.62it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0228, Accuracy: 9923/10000 (99.23%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.019239870831370354 batch_id=1999 Accuracy=97.46: 100%|██████████| 2000/2000 [00:47<00:00, 42.02it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0234, Accuracy: 9926/10000 (99.26%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.2787499725818634 batch_id=1999 Accuracy=97.58: 100%|██████████| 2000/2000 [00:48<00:00, 41.45it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0227, Accuracy: 9924/10000 (99.24%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.21195459365844727 batch_id=1999 Accuracy=97.75: 100%|██████████| 2000/2000 [00:47<00:00, 41.72it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 9928/10000 (99.28%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.02348860166966915 batch_id=1999 Accuracy=97.74: 100%|██████████| 2000/2000 [00:47<00:00, 41.77it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0196, Accuracy: 9945/10000 (99.45%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.007239468861371279 batch_id=1999 Accuracy=97.86: 100%|██████████| 2000/2000 [00:47<00:00, 42.05it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0201, Accuracy: 9944/10000 (99.44%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.04992241784930229 batch_id=1999 Accuracy=97.84: 100%|██████████| 2000/2000 [00:47<00:00, 41.98it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 9943/10000 (99.43%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.01386117935180664 batch_id=1999 Accuracy=97.98: 100%|██████████| 2000/2000 [00:47<00:00, 42.06it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0188, Accuracy: 9934/10000 (99.34%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.031740952283144 batch_id=1999 Accuracy=98.09: 100%|██████████| 2000/2000 [00:47<00:00, 41.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0177, Accuracy: 9940/10000 (99.40%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zpq_4PFf4pQ",
        "colab_type": "text"
      },
      "source": [
        "**Target:**\n",
        "1. Getting an accuracy of 99.4 with less than 10k parametrs under 15 epochs.\n",
        "Results:\n",
        "1. Parameters: 90736 \n",
        "2. Best Train Accuracy:98.09\n",
        "3. Best Test Accuracy: 99.45\n",
        "\n",
        "**Analysis:**\n",
        "\n",
        "1. The model is doing pretty well not overfitting as well as getting consistent accuracy.\n",
        "\n",
        "2. Seeing image sample ,we get to know we can add slight rotation for better accuracy.\n",
        "3. Doing image augmentation and choosing the step size of LR schedular wisely can improve the accuracy.\n",
        "\n",
        "4. Adding too many image augmentation techniques work in a negative way reduces the accuracy or have not much impact"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}